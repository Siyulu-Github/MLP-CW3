### model
model_name_or_path: Qwen/Qwen2.5-VL-3B-Instruct
adapter_name_or_path: saves/qwen2.5_vl-3b_cold_start/lora/sft   #对冷启动后的模型训练
image_max_pixels: 12845056
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 32   #original 32
lora_target: all #["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj"] #original all
lora_alpha: 128 #32 
lora_dropout: 0.05
deepspeed: examples/deepspeed/zero2.json
ddp_find_unused_parameters: false


### dataset
dataset: percive_sft_train
eval_dataset: percive_sft_val
template: qwen2_vl
cutoff_len: 8192
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4


### output
output_dir: saves/qwen2.5_vl-3b_cold_start_stage2/lora/sft
logging_steps: 10
save_steps: 40 #40  #500
plot_loss: true
overwrite_output_dir: true
save_only_model: false

### train
per_device_train_batch_size: 16 #coldstart #percive 16
gradient_accumulation_steps: 1
learning_rate: 1.0e-4
num_train_epochs: 5 #cold_start 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null
seed: 42
gradient_checkpointing: true


### eval
val_size: 0
per_device_eval_batch_size: 16
eval_strategy: steps
eval_steps: 40
eval_accumulation_steps: 1
compute_accuracy: true
temperature: 1

